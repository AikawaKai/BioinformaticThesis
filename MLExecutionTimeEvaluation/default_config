# gaussprPoly
algorithm <- "gaussprPoly";
defGrid <- data.frame(degree=1, scale=1);
doclink: https://cran.r-project.org/web/packages/kernlab/kernlab.pdf (pag 9)
note: -

# xgbLinear
algorithm <- "xgbLinear";
defGrid <- data.frame(nrounds=15, lambda=1, alpha=0, eta=0.3);
doclink: https://cran.r-project.org/web/packages/xgboost/xgboost.pdf (pag 38)
note: Lower value for eta implies larger value for nrounds: low eta value means model more robust to overfitting but slower to compute. Default: 0.3

# AdaBoost.M1
algorithm <- "AdaBoost.M1"; 
data.frame(mfinal = c(100), maxdepth = c(30), coeflearn = c("Breiman"));
doclink: https://cran.r-project.org/web/packages/adabag/adabag.pdf (pag 9) -  (maxdepth param) https://cran.r-project.org/web/packages/rpart/rpart.pdf (pag 22)
note: -

# svmRadial
algorithm <- "svmRadial";
defGrid <- data.frame(C = c(1), sigma = c(1));
doclink: https://cran.r-project.org/web/packages/kernlab/kernlab.pdf (pag 56) https://cran.r-project.org/web/packages/kernlab/kernlab.pdf#Rfn.rbfdot (for sigma param, pag 9)
note: -

# random forest
algorithm <- "rf";
defGrid <- data.frame(mtry = sqrt(p)); # p = number of predictors, in our specific case p = 15752, so mtry = 126
doclink: https://cran.r-project.org/web/packages/randomForest/randomForest.pdf (pag 18)
note: -

# LogitBoost
algorithm <- "LogitBoost";
defGrid <- data.frame(nIter = ncol(x)); # in our specific case ncol(x) = 15752
doclink: https://cran.r-project.org/web/packages/caTools/caTools.pdf (pag 10)
note: -

# C5.0
algorithm <- "C5.0";
defGrid <- data.frame(trials = c(1), model = c("tree"), winnow=c(FALSE));
doclink: https://cran.r-project.org/web/packages/C50/C50.pdf (pag 2)
note: -



